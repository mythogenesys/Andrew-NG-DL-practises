{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4634c2a5-d08c-4145-9af0-5d8063ab5b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DeepNeuralNetwork:\n",
    "    def __init__(self, layer_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False, optimizer=\"gd\", beta=0.9, beta1=0.9, beta2=0.999, epsilon=1e-8, mini_batch_size=64):\n",
    "        self.layer_dims = layer_dims\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.print_cost = print_cost\n",
    "        self.optimizer = optimizer\n",
    "        self.beta = beta\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.parameters = self.initialize_parameters_deep()\n",
    "        self.t = 0\n",
    "        self.v, self.s = None, None\n",
    "        \n",
    "        # Initialize the optimizer\n",
    "        if self.optimizer == \"momentum\":\n",
    "            self.v = self.initialize_velocity(self.parameters)\n",
    "        elif self.optimizer == \"adam\":\n",
    "            self.v, self.s = self.initialize_adam(self.parameters)\n",
    "\n",
    "    def initialize_parameters_deep(self):\n",
    "        np.random.seed(3)\n",
    "        parameters = {}\n",
    "        L = len(self.layer_dims)\n",
    "\n",
    "        for l in range(1, L):\n",
    "            parameters['W' + str(l)] = np.random.randn(self.layer_dims[l], self.layer_dims[l-1]) * 0.01\n",
    "            parameters['b' + str(l)] = np.zeros((self.layer_dims[l], 1))\n",
    "            \n",
    "            assert(parameters['W' + str(l)].shape == (self.layer_dims[l], self.layer_dims[l-1]))\n",
    "            assert(parameters['b' + str(l)].shape == (self.layer_dims[l], 1))\n",
    "        \n",
    "        return parameters\n",
    "\n",
    "    def linear_forward(self, A, W, b):\n",
    "        Z = np.dot(W, A) + b\n",
    "        cache = (A, W, b)\n",
    "        return Z, cache\n",
    "\n",
    "    def sigmoid(self, Z):\n",
    "        A = 1 / (1 + np.exp(-Z))\n",
    "        cache = Z\n",
    "        return A, cache\n",
    "\n",
    "    def relu(self, Z):\n",
    "        A = np.maximum(0, Z)\n",
    "        cache = Z\n",
    "        return A, cache\n",
    "\n",
    "    def linear_activation_forward(self, A_prev, W, b, activation):\n",
    "        if activation == \"sigmoid\":\n",
    "            Z, linear_cache = self.linear_forward(A_prev, W, b)\n",
    "            A, activation_cache = self.sigmoid(Z)\n",
    "        elif activation == \"relu\":\n",
    "            Z, linear_cache = self.linear_forward(A_prev, W, b)\n",
    "            A, activation_cache = self.relu(Z)\n",
    "        cache = (linear_cache, activation_cache)\n",
    "        return A, cache\n",
    "\n",
    "    def L_model_forward(self, X):\n",
    "        caches = []\n",
    "        A = X\n",
    "        L = len(self.parameters) // 2\n",
    "\n",
    "        for l in range(1, L):\n",
    "            A_prev = A \n",
    "            A, cache = self.linear_activation_forward(A_prev, self.parameters['W' + str(l)], self.parameters['b' + str(l)], 'relu')\n",
    "            caches.append(cache)\n",
    "\n",
    "        AL, cache = self.linear_activation_forward(A, self.parameters['W' + str(L)], self.parameters['b' + str(L)], 'sigmoid')\n",
    "        caches.append(cache)\n",
    "        return AL, caches\n",
    "\n",
    "    def compute_cost(self, AL, Y):\n",
    "        m = Y.shape[1]\n",
    "        # Clip AL to avoid log(0)\n",
    "        AL = np.clip(AL, 1e-10, 1 - 1e-10)\n",
    "        cost = -(1/m) * np.sum(Y * np.log(AL) + (1-Y) * np.log(1-AL))\n",
    "        cost = np.squeeze(cost)\n",
    "        return cost\n",
    "\n",
    "    def linear_backward(self, dZ, cache):\n",
    "        A_prev, W, b = cache\n",
    "        m = A_prev.shape[1]\n",
    "\n",
    "        dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "        db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA_prev = np.dot(W.T, dZ)\n",
    "        return dA_prev, dW, db\n",
    "\n",
    "    def relu_backward(self, dA, cache):\n",
    "        Z = cache\n",
    "        dZ = np.array(dA, copy=True)\n",
    "        dZ[Z <= 0] = 0\n",
    "        return dZ\n",
    "\n",
    "    def sigmoid_backward(self, dA, cache):\n",
    "        Z = cache\n",
    "        s = 1 / (1 + np.exp(-Z))\n",
    "        dZ = dA * s * (1 - s)\n",
    "        return dZ\n",
    "\n",
    "    def linear_activation_backward(self, dA, cache, activation):\n",
    "        linear_cache, activation_cache = cache\n",
    "        if activation == \"relu\":\n",
    "            dZ = self.relu_backward(dA, activation_cache)\n",
    "        elif activation == \"sigmoid\":\n",
    "            dZ = self.sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = self.linear_backward(dZ, linear_cache)\n",
    "        return dA_prev, dW, db\n",
    "\n",
    "    def L_model_backward(self, AL, Y, caches):\n",
    "        grads = {}\n",
    "        L = len(caches)\n",
    "        m = AL.shape[1]\n",
    "        Y = Y.reshape(AL.shape)\n",
    "\n",
    "        dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "        current_cache = caches[L-1]\n",
    "        grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = self.linear_activation_backward(dAL, current_cache, activation='sigmoid')\n",
    "\n",
    "        for l in reversed(range(L-1)):\n",
    "            current_cache = caches[l]\n",
    "            dA_prev_temp, dW_temp, db_temp = self.linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation='relu')\n",
    "            grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "            grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "            grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def update_parameters_with_gd(self, parameters, grads):\n",
    "        L = len(parameters) // 2\n",
    "        for l in range(1, L + 1):\n",
    "            parameters[\"W\" + str(l)] -= self.learning_rate * grads[\"dW\" + str(l)]\n",
    "            parameters[\"b\" + str(l)] -= self.learning_rate * grads[\"db\" + str(l)]\n",
    "        return parameters\n",
    "\n",
    "    def random_mini_batches(self, X, Y, seed=0):\n",
    "        np.random.seed(seed)\n",
    "        m = X.shape[1]\n",
    "        mini_batches = []\n",
    "        permutation = list(np.random.permutation(m))\n",
    "        shuffled_X = X[:, permutation]\n",
    "        shuffled_Y = Y[:, permutation].reshape((1, m))\n",
    "\n",
    "        num_complete_minibatches = m // self.mini_batch_size\n",
    "        for k in range(0, num_complete_minibatches):\n",
    "            mini_batch_X = shuffled_X[:, k * self.mini_batch_size: (k + 1) * self.mini_batch_size]\n",
    "            mini_batch_Y = shuffled_Y[:, k * self.mini_batch_size: (k + 1) * self.mini_batch_size]\n",
    "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "            mini_batches.append(mini_batch)\n",
    "\n",
    "        if m % self.mini_batch_size != 0:\n",
    "            mini_batch_X = shuffled_X[:, num_complete_minibatches * self.mini_batch_size:]\n",
    "            mini_batch_Y = shuffled_Y[:, num_complete_minibatches * self.mini_batch_size:]\n",
    "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "            mini_batches.append(mini_batch)\n",
    "\n",
    "        return mini_batches\n",
    "\n",
    "    def initialize_velocity(self, parameters):\n",
    "        L = len(parameters) // 2\n",
    "        v = {}\n",
    "        for l in range(1, L + 1):\n",
    "            v[\"dW\" + str(l)] = np.zeros(parameters[\"W\" + str(l)].shape)\n",
    "            v[\"db\" + str(l)] = np.zeros(parameters[\"b\" + str(l)].shape)\n",
    "        return v\n",
    "\n",
    "    def update_parameters_with_momentum(self, parameters, grads, v):\n",
    "        L = len(parameters) // 2\n",
    "        for l in range(1, L + 1):\n",
    "            v[\"dW\" + str(l)] = self.beta * v[\"dW\" + str(l)] + (1 - self.beta) * grads[\"dW\" + str(l)]\n",
    "            v[\"db\" + str(l)] = self.beta * v[\"db\" + str(l)] + (1 - self.beta) * grads[\"db\" + str(l)]\n",
    "            parameters[\"W\" + str(l)] -= self.learning_rate * v[\"dW\" + str(l)]\n",
    "            parameters[\"b\" + str(l)] -= self.learning_rate * v[\"db\" + str(l)]\n",
    "        return parameters, v\n",
    "\n",
    "    def initialize_adam(self, parameters):\n",
    "        L = len(parameters) // 2\n",
    "        v = {}\n",
    "        s = {}\n",
    "        for l in range(1, L + 1):\n",
    "            v[\"dW\" + str(l)] = np.zeros(parameters[\"W\" + str(l)].shape)\n",
    "            v[\"db\" + str(l)] = np.zeros(parameters[\"b\" + str(l)].shape)\n",
    "            s[\"dW\" + str(l)] = np.zeros(parameters[\"W\" + str(l)].shape)\n",
    "            s[\"db\" + str(l)] = np.zeros(parameters[\"b\" + str(l)].shape)\n",
    "        return v, s\n",
    "\n",
    "    def update_parameters_with_adam(self, parameters, grads, v, s, t):\n",
    "        L = len(parameters) // 2\n",
    "        v_corrected = {}\n",
    "        s_corrected = {}\n",
    "        for l in range(1, L + 1):\n",
    "            v[\"dW\" + str(l)] = self.beta1 * v[\"dW\" + str(l)] + (1 - self.beta1) * grads[\"dW\" + str(l)]\n",
    "            v[\"db\" + str(l)] = self.beta1 * v[\"db\" + str(l)] + (1 - self.beta1) * grads[\"db\" + str(l)]\n",
    "            v_corrected[\"dW\" + str(l)] = v[\"dW\" + str(l)] / (1 - self.beta1 ** t)\n",
    "            v_corrected[\"db\" + str(l)] = v[\"db\" + str(l)] / (1 - self.beta1 ** t)\n",
    "            s[\"dW\" + str(l)] = self.beta2 * s[\"dW\" + str(l)] + (1 - self.beta2) * (grads[\"dW\" + str(l)] ** 2)\n",
    "            s[\"db\" + str(l)] = self.beta2 * s[\"db\" + str(l)] + (1 - self.beta2) * (grads[\"db\" + str(l)] ** 2)\n",
    "            s_corrected[\"dW\" + str(l)] = s[\"dW\" + str(l)] / (1 - self.beta2 ** t)\n",
    "            s_corrected[\"db\" + str(l)] = s[\"db\" + str(l)] / (1 - self.beta2 ** t)\n",
    "            parameters[\"W\" + str(l)] -= self.learning_rate * (v_corrected[\"dW\" + str(l)] / (np.sqrt(s_corrected[\"dW\" + str(l)]) + self.epsilon))\n",
    "            parameters[\"b\" + str(l)] -= self.learning_rate * (v_corrected[\"db\" + str(l)] / (np.sqrt(s_corrected[\"db\" + str(l)]) + self.epsilon))\n",
    "        return parameters, v, s\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        costs = []\n",
    "        seed = 10\n",
    "        for i in range(self.num_iterations):\n",
    "            seed += 1\n",
    "            minibatches = self.random_mini_batches(X, Y, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "                AL, caches = self.L_model_forward(minibatch_X)\n",
    "                cost = self.compute_cost(AL, minibatch_Y)\n",
    "                grads = self.L_model_backward(AL, minibatch_Y, caches)\n",
    "\n",
    "                if self.optimizer == \"gd\":\n",
    "                    self.parameters = self.update_parameters_with_gd(self.parameters, grads)\n",
    "                elif self.optimizer == \"momentum\":\n",
    "                    self.parameters, self.v = self.update_parameters_with_momentum(self.parameters, grads, self.v)\n",
    "                elif self.optimizer == \"adam\":\n",
    "                    self.t += 1\n",
    "                    self.parameters, self.v, self.s = self.update_parameters_with_adam(self.parameters, grads, self.v, self.s, self.t)\n",
    "\n",
    "            if self.print_cost and i % 100 == 0:\n",
    "                print(f\"Cost after iteration {i}: {cost}\")\n",
    "                costs.append(cost)\n",
    "\n",
    "        if self.print_cost:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.plot(np.squeeze(costs))\n",
    "            plt.ylabel('cost')\n",
    "            plt.xlabel('iterations (per 100)')\n",
    "            plt.title(f\"Learning rate = {self.learning_rate}\")\n",
    "            plt.show()\n",
    "\n",
    "        return self.parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4476fd18-fade-4536-ab1b-a2064ee41c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Bank Marketing Dataset\n",
    "data = pd.read_csv('/Users/mohanarangand/Desktop/Andrew NG DL notebook/Course 2/Week 2/W2A1/Extra NBs/Datasets/bank-additional-full.csv', sep=';')\n",
    "\n",
    "# Encode the target variable ('yes' -> 1, 'no' -> 0)\n",
    "data['y'] = data['y'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "data = pd.get_dummies(data, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# Split features and target\n",
    "X = data.drop('y', axis=1).values\n",
    "Y = data['y'].values.reshape(1, -1)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y.T, test_size=0.2, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96abf3f9-ed2e-4709-849e-43a879881a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.22931011664958043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_s/ghww3yps68v7gtbrl1t6b9nw0000gn/T/ipykernel_17037/3334636420.py:49: RuntimeWarning: overflow encountered in exp\n",
      "  A = 1 / (1 + np.exp(-Z))\n",
      "/var/folders/_s/ghww3yps68v7gtbrl1t6b9nw0000gn/T/ipykernel_17037/3334636420.py:126: RuntimeWarning: invalid value encountered in divide\n",
      "  dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
      "/var/folders/_s/ghww3yps68v7gtbrl1t6b9nw0000gn/T/ipykernel_17037/3334636420.py:107: RuntimeWarning: overflow encountered in exp\n",
      "  s = 1 / (1 + np.exp(-Z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 100: nan\n",
      "Cost after iteration 200: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Initialize and train the model\u001b[39;00m\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m DeepNeuralNetwork(layer_dims, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0075\u001b[39m, num_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3000\u001b[39m, print_cost\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m parameters, costs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Forward propagation on the test set\u001b[39;00m\n\u001b[1;32m     14\u001b[0m AL_test, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mL_model_forward(X_test)\n",
      "Cell \u001b[0;32mIn[2], line 226\u001b[0m, in \u001b[0;36mDeepNeuralNetwork.fit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    224\u001b[0m AL, caches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL_model_forward(minibatch_X)\n\u001b[1;32m    225\u001b[0m cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_cost(AL, minibatch_Y)\n\u001b[0;32m--> 226\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL_model_backward(AL, minibatch_Y, caches)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_parameters_with_gd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters, grads)\n",
      "Cell \u001b[0;32mIn[2], line 126\u001b[0m, in \u001b[0;36mDeepNeuralNetwork.L_model_backward\u001b[0;34m(self, AL, Y, caches)\u001b[0m\n\u001b[1;32m    123\u001b[0m m \u001b[38;5;241m=\u001b[39m AL\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    124\u001b[0m Y \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mreshape(AL\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 126\u001b[0m dAL \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m (np\u001b[38;5;241m.\u001b[39mdivide(Y, AL) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mdivide(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m Y, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m AL))\n\u001b[1;32m    127\u001b[0m current_cache \u001b[38;5;241m=\u001b[39m caches[L\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    128\u001b[0m grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdA\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(L\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)], grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdW\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(L)], grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(L)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_activation_backward(dAL, current_cache, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train).T\n",
    "X_test = scaler.transform(X_test).T\n",
    "\n",
    "# Define the layer dimensions (input layer size should match the number of features)\n",
    "layer_dims = [X_train.shape[0], 20, 7, 5, 1]\n",
    "\n",
    "# Initialize and train the model\n",
    "model = DeepNeuralNetwork(layer_dims, learning_rate=0.0075, num_iterations=3000, print_cost=True, optimizer=\"adam\")\n",
    "parameters, costs = model.fit(X_train, Y_train.T)\n",
    "\n",
    "# Forward propagation on the test set\n",
    "AL_test, _ = model.L_model_forward(X_test)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "predictions = (AL_test > 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predictions == Y_test)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84014690-63d6-4dc7-a5f2-f3790b23ca19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
